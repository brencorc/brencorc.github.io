---
layout: page
title: CS 109
---


# CS 109 Final Review

[CS 109 Course Website (Spring 2017)](https://web.stanford.edu/class/archive/cs/cs109/cs109.1176/) 

[CS 109 Course Reader](https://web.stanford.edu/class/archive/cs/cs109/cs109.1176/courseReader.html) 


## Part 1: Combinatorics and Event-Based Probability


### Combinatorics

**permutation**: ordered arrangement of n distinct objects: $$ n! $$ permutations

  if some are indistinguishable: $$ \frac{n!}{n_1! n_2! ... n_r! } $$

**combinations**: unordered selection of r objects from a set of n distinct objects

$$ \frac{n!}{r! (n-r)! } = {n \choose r} $$

**divider method**: when you want to place n indistinguishable objects into r containers


imagine the divider between containers is an object itself, then do permutations with indistinguishable objects


$$ \frac{ (n+ r - 1)!}{n! (r-1)! } = { n+r-1 \choose r-1 } $$


### Event-based probability

sample space S (set of all possible outcomes), event space E (some subset of S)

$$P(E) = \lim_{n \to \infty} \frac{n(E)}{n} $$


#### probability of OR

E and F mutually exclusive? $$ P(E \cup F) = P(E) + P(F) $$

not? **Principle of Inclusion-Exclusion**: $$ P(E \cup F) = P(E) + P(F) - P(EF) $$


#### probability of AND

independent? $$ P(EF) = P(E)P(F) $$

not independent? use conditional probability/total probability

**De Morgan’s laws** (switch ANDs to ORs)

$$ P((E \cap F)^C) = P(E^C \cup F^C) $$

$$ P((E \cup F)^C) = P(E^C \cap F^C) $$

example: probability roll 3 on Die 1 OR a 4 on Die 2?

E = 3 on Die 1, F = 4 on Die 2

E and F are independent events, so we want to convert the OR to an AND

$$ P(E \cup F) = 1-P((E \cup F)^C) = 1-P(E^C \cap F^C) = 1-P(E^C)P(F^C) $$


#### conditional probability

**conditional probability**

 $$ P(E | F) = \frac{P(EF)}{P(F)} $$

**Chain Rule (rearrange definition)**

$$P(EF) = P(E|F)P(F) $$

**Law of Total Probability** 

$$ P(E) = P(E|F)P(F) + P(E | F^C)P(F^C) $$ 

**Bayes’ theorem**

$$ P(E|F) = \frac{ P(F|E)P(E) } { P(F) } $$


#### conditional independence

_conditioning on a new event can change the independence or mutual exclusion relationship between two events_

independence 
$$P(EF) = P(E)P(F) $$
 
conditional independence
$$P(EF |G) = P(E| G)P(F| G)$$

explaining away (V structure)


## Part 2: Random Variables


### Random Variables Basics 

**Discrete**

PMF $$ p_Y (k) = P(Y = k) $$

CMF $$F_Y (k) = P(Y \leq k) 	$$			

ex: Bernoulli, Binomial, Poisson, Geometric, Negative Binomial

**Continuous**

PDF $$\int_a^b f_Y (k) dk = P(a \leq Y \leq b) $$

CDF $$ F_Y(k) = P(Y \leq k) $$

ex: Exponential, Uniform, Normal

**expectation/variance**

expectation 	$$ E[X] = \Sigma x P(X=x) $$

variance 	$$ Var(X) = E[(X-\mu)^2] = E[X^2] - E[X]^2 $$

$$ E[aX+b] = aE[X] + b $$

$$ Var(aX+b) = a^2 Var(X) $$


### Parametric Distributions


#### Bernoulli trials (heads/tails)


|**Bernoulli**|result of coin flip|
|**Binomial**|number of heads in n coin flips|
|**Geometric**|number of coin flips until the first heads|
|**Negative Binomial**|number of coin flips until r successes|




#### Poisson processes



*   approximation of Bernoulli where n is large, p is small, and np is moderate
*   chop up time interval into lots of small “trials”
*   ex: earthquakes, decays of radioactive sample, customers to call center

|**Poisson**|number of earthquakes in a year|
|**Exponential**|time until next earthquake (memoryless)|





#### Normal (Gaussian)



*   distribution of sum of random variables (CLT) → often occurs in nature
*   best estimate given only mean/variance
*   if $$X \sim N(\mu, \sigma^2)$$ and $$Y = aX+b$$, then $$ Y \sim N(a \mu + b, a^2 \sigma^2) $$


### Multivariate Random Variables


#### Joint Distributions

**joint distribution** $$ p_{X,Y}(x,y)=P(X=x,Y=y) $$

example: multinomial distribution

**marginal distribution:** we don’t care about one of the variables anymore so sum over it

$$ p_X(a) = \sum_y P_{X,Y}(a, y) $$

$$ p_Y(b) = \sum_x P_{X,Y}(x, b) $$

**conditional distribution:** fix one of the variables, normalize with marginal

$$p_{X|Y}(x | b) = \frac{ p_{X, Y}(x, b) }{p_Y(b)} $$

**independent variables:** can you factor the joint PMF/PDF into product of simple PMF/PDFs (remember distinction between _events_ and _variables_)

$$f_{X,Y} (x, y) = g(x)h(y) $$


#### Covariance/Correlation

**covariance**: measures the extent to which the deviation of one variable from its mean matches the deviation of the other from its mean

 $$ Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]$$

**correlation**: normalizes Cov to [0, 1]

 $$\rho(X, Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} $$

independence implies $$\rho=0$$, but $$ \rho = 0 $$ does NOT imply independence


#### Expectation

**sum of expectation**

**$$ E[\sum_i X_i] = \sum_i E[X_i] $$**

**conditional expectation**: fix one variable and calculate expectation

$$ E[X | Y=y] = \sum_x x p_{X|Y}(x|y) $$

**law of total expectation**: similar to law of total probability (can split up expectation into different cases)

$$ E[X] = E[E[X|Y]] = \sum_y E[X | Y=y] P(Y=y) $$


## Part 3: Sampling and Inference


### Sampling/Estimators



*   population (underlying) distribution F $$ E[F] = \mu $$, $$Var(F) = \sigma^2 $$
*   consider n random variables $$ X_1, ..., X_n $$ that are IID from F
*   we call the sequence $$ X_1, ..., X_n $$ a **sample** from distribution F

we have a sample, and seek to “estimate” properties of the population distribution

$$ \hat{\theta} $$ is a random variable, having different values for different samples

**unbiased estimator** $$ E[\hat{\theta}] = \theta $$

**consistent estimator** 
$$ \lim_{n \to \infty} P(| \hat{\theta}_n - \theta | < \varepsilon) = 1$$ 
for all 
$$ \varepsilon > 0 $$

**sample mean $$ \bar{X} = \sum_i \frac{X_i}{n} $$**

**sample variance $$ S^2 = \sum_i \frac{(X_i - \bar{X})^2}{n-1} $$**

**variance of sample mean, standard error**

**$$ Var(\bar{X}) = \frac{\sigma^2}{n} \approx \frac{S^2}{n} $$**

$$ Std(\bar{X}) \approx \sqrt{ \frac{S^2}{n}} $$

“Let’s say the sample of happiness has $$n = 200$$ people. The sample mean is $$\bar{X} = 83$$ and the sample variance is $$S^2 = 450$$. We can now calculate the standard error of our estimate of the mean to be 1.5. When we report our results we will say that the average happiness score in Bhutan is 83 ± 1.5 with variance 450.”


### Hypothesis Testing

**statistic**: quantity computed from the values in a sample; used to estimate population parameter, describe a sample, or evaluate a hypothesis

**statistical inference**: the process of deducing properties of the underlying distribution

**statistical hypothesis**: any conjecture concerning the underlying distribution of a sample (eg: the population mean is x, the underlying distributions of two samples have different means)

**statistical hypothesis testing**: state one hypothesis and see whether it is tenable; nullify the null hypothesis (proof by contradiction)

**p-value**: probability of obtaining test results at least as extreme as the results actually observed, assuming the null hypothesis is true

**statistically significant**: if p-value is less than significance level (alpha); causes us to reject the null hypothesis

**transposed conditional fallacy**: P(observed \| hypothesis) ≠ P(hypothesis \| observed)


#### single-sample hypothesis testing (μ ≠ μ0)

**proportion example**: A cereal company claims there is a voucher in 20% of boxes. We collect a sample of 65 boxes and find 11 vouchers. Based on this sample, is there statistical evidence that the proportion of boxes with vouchers is &lt;20%?

H0: cereal company is telling truth p = 0.2

**mean example**: The mean volume of milk in a milk plant’s containers must be 128oz to be compliant. We take 12 containers to sample and measure sample mean 127.2oz and sample stdev 2.1oz. Based on this sample, is there statistical evidence that the mean volume of milk is &lt;128oz?

H0 : milk plant is compliant mu = 128


#### two-sample hypothesis testing (μ1 ≠ μ2)

**proportion example**: A vet thinks a disease affects male cats more than female cats. We get a sample where 24 out of 259 male cats and 14 out of 241 female cats have the disease. Is there sufficient evidence to conclude a larger proportion of male cats have the disease? 

H0: no difference in mean  p1 = p2

**mean example**: We are trying to figure out if people are happier in Bhutan or Nepal. We collect n1=200 individual happiness scores from Bhutan and n2=300 scores from Nepal. Based on these two samples, is there sufficient evidence that the mean happiness score in Nepal is greater than that of Bhutan?

H0: no difference in mean  μ1 = μ2

**can use t-test, confidence intervals, bootstrapping**


### Central Theorems

**Markov and Chebychev bounds**

**Markov**: if X is nonneg RV: $$ P (x \geq a) \leq \frac{E[X]}{a} $$

**Chebychev**: 
$$ P(|X-\mu| \geq k) \leq \frac{\sigma^2}{k^2} $$

**weak and strong law of large numbers**

for IID RVs $$ X_1, ..., X_n $$ where $$E[X_i] = \mu$$ and $$Var(X_i) = \sigma^2$$. Then for any $$\varepsilon > 0$$:

$$ \lim_{n \to \infty} P(|\bar{X}_n - \mu | > \varepsilon) = 0 $$

**Central Limit Theorem**

for IID RVs $$ X_1, ..., X_n $$ where $$E[X_i] = \mu$$ and $$Var(X_i) = \sigma^2$$,  as $$ n \to \infty $$

$$ \bar{X} \sim N(\mu, \frac{\sigma^2}{n}) $$

$$ \sum_i X_i \sim N(n \mu, n  \sigma^2) $$


### Parameter Estimation

**the situation**



*   we have a sample (set of examples from the same underlying distribution), but we don’t know what the underlying distribution is
*   we are trying to estimate the underlying distribution
    *   manually specify the model (parametric family)
    *   estimate the values of those parameters from the data

**Maximum Likelihood Estimator (MLE)**

likelihood: 
$$ L(\theta) = \prod_i f(X_i | \theta) $$

maximization: 
$$ \hat{\theta} = \arg_\theta \max L(\theta) $$

log likelihood: 
$$ LL(\theta) = \log \prod_i f(X_i | \theta) = \sum_i \log f(X_i | \theta) $$

**Maximum A Posteriori Estimator (MAP)**

$$ \theta_{MAP} = \arg_\theta\max f(\theta | X_1, ..., X_n) =  \arg_\theta\max \frac{f(X_1, ..., X_n) g(\theta)}{h(X_1, ..., X_n)} =  \arg_\theta\max \prod_i f(X_i|\theta) g(\theta) $$

using Bayes Theorem, and since X’s are IID and h doesn’t depend on theta

what do we use for g(theta) prior?

conjugate prior: resulting posterior distribution will have same functional form

Beta (Bernoulli, Binomial)		

Beta(a, b): like seeing a heads and b tails

Gamma (Poisson, Exponential)

Gamma(k, theta): like seeing k imaginary events during theta time periods


### Machine Learning

**Naive Bayes Classifier**

$$ \hat{y} = \arg_y \max P(X|Y)P(Y) = \arg_y \max \prod_i p(X_i = x_i | Y=y)p(Y=y) $$

$$ = \arg_y \max \sum_i \log p(X_i = x_i | Y=y) + \log p(Y=y) $$

**Logistic Regression Classifier**

assumes:

$$P(Y=1 | X=x) = \sigma(\theta^T x) $$

can interpret each label as a Bernoulli RV:

$$ Y \sim Bern(p) $$ where $$ p = \sigma(\theta^T x) $$



1. write down log likelihood function

$$ LL(\theta) = \sum_i (1-y^{(i)}) \log(1- \sigma(\theta^T x^{(i)})) + y^{(i)} \log ( \sigma(\theta^T x^{(i)}) ) $$



2. find theta that maximizes LL using gradient ascent

		

			

		
